# Author: Qian Liu
# Email: liu.qian.pro@gmail.com

import torch, numpy as np
import torch.nn.functional as F
from sam2.build_sam import build_sam2
from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator
import argparse
from tqdm import tqdm
from PIL import Image
from pathlib import Path
import glob, json, os


def get_sam2_masks(image, device='cuda:0'):
    """
    Get SAM2 masks for a given image.
    :param image: Input image as a numpy array, (H, W, C).
    :param device: GPU device to use for computation.
    :return: List of masks generated by SAM2.
    """
    sam2_checkpoint = "checkpoints/sam2.1_hiera_large.pt"
    # sam2_checkpoint = "checkpoints/sam2.1_hiera_base_plus.pt"
    model_cfg = "configs/sam2.1/sam2.1_hiera_l.yaml"
    # model_cfg = "configs/sam2.1/sam2.1_hiera_b+.yaml"

    sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)

    # mask_generator = SAM2AutomaticMaskGenerator(sam2)
    mask_generator = SAM2AutomaticMaskGenerator(
        model=sam2,
        points_per_side=128,
        points_per_batch=128,
        pred_iou_thresh=0.7,
        stability_score_thresh=0.92,
        stability_score_offset=0.7,
        crop_n_layers=1,
        box_nms_thresh=0.7,
        crop_n_points_downscale_factor=2,
        min_mask_region_area=25.0,
        use_m2m=True,
    )

    masks = mask_generator.generate(image)
    masks = [mask['segmentation'] for mask in masks]  # Extract only the segmentation masks
    masks = torch.tensor(masks)  # Convert to tensor

    del sam2
    del mask_generator
    torch.cuda.empty_cache()

    return masks


def get_sam2_masks_from_path(args):
    """
    Get SAM2 masks for images in a specified directory.
    :param args: Arguments containing input path and GPU ID.
    :return: List of masks for each image.
    """
    device = torch.device(f"cuda:{args.gpu_id}" if torch.cuda.is_available() else "cpu")

    image_paths = sorted(glob.glob(f"{args.input_path}/*.jpg"))
    print(f"Found {len(image_paths)} images in {args.input_path}")
    # save the extracted masks
    save_path = args.save_path
    os.makedirs(save_path, exist_ok=True)

    for image_path in tqdm(image_paths, desc="Processing images"):
        image_path = Path(image_path)
        image = np.array(Image.open(image_path).convert("RGB"))
        masks = get_sam2_masks(image, device=device)
        
        print(f"Extracted masks from {image_path.name}, shape: {masks.shape}, dtype: {masks.dtype}")
        save_file_path = os.path.join(save_path, f"{image_path.stem}.pt")
        torch.save(masks, save_file_path)


def save_SRMR(clip_vis_feature: Tensor, clip_text_features: Tensor, sam2_masks: Tensor, save_name, args):
    """
    Save the SRMR (Semantic Relevancy Map Regularization) map for a given image feature and
    its related scene classes features.
    :param vis_feature: visual features of the image (H, W, D).
    :param clip_text_features: CLIP text features of the scene classes.
    :param sam2_masks: SAM2 masks for the image.
    :param save_name: Name to save the SRMR map.
    :param args: Arguments containing GPU ID and other configurations.
    :return: A tensor of shape (H, W) with the regularized relavancy map of each pixel.
    """
    H, W =  clip_vis_feature.size(0), clip_vis_feature.size(1) # [height, width]
    device = clip_vis_feature.device
    
    # Get Clip features 
    clip_vis_feature = clip_vis_feature.reshape(-1, clip_vis_feature.size(-1)) # [N1, D], N1 is H*W, D is the feature dimension
    clip_text_features_normalized = F.normalize(clip_text_features, dim=1) # [N2, D], N2 is the number of scene classes
    clip_vis_feature_normalized = F.normalize(clip_vis_feature, dim=1) # [N1, D], N1 is 1 or H*W, D is the feature dimension
    # Compute cosine similarity
    relevancy_map = torch.mm(clip_vis_feature_normalized, clip_text_features_normalized.T) # [N1,N2]        
    p_class = F.softmax(relevancy_map, dim=1) # [N1,N2]
    class_index = torch.argmax(p_class, dim=-1) # [N1]
    pred_index = class_index.reshape(H, W).unsqueeze(0) # [1,H,W]

    # Refine SAM2 masks using the predicted class_index  
    sam_refined_pred = torch.zeros((pred_index.shape[1], pred_index.shape[2]),
                                   dtype=torch.long).to(device)

    for ann in sam2_masks:
        cur_mask = ann.squeeze()  # [H, W], current mask for the annotation                  
        sub_mask = pred_index.squeeze().clone()
        sub_mask[~cur_mask] = 0
        # .view(-1) collapses all dimensions into a single dimension, It is equivalent to tensor.reshape(-1).
        flat_sub_mask = sub_mask.clone().view(-1)           
        flat_sub_mask = flat_sub_mask[flat_sub_mask!=0]
        
        if len(flat_sub_mask) != 0:                         
            unique_elements, counts = torch.unique(flat_sub_mask, return_counts=True)  
            most_common_element = unique_elements[int(counts.argmax().item())]  
        else:                                               
            continue 

        sam_refined_pred[cur_mask] = most_common_element  
    
    # save the extracted masks
    save_path = args.save_path
    os.makedirs(save_path, exist_ok=True)
    save_file_path = os.path.join(save_path, f"{save_name}.pt")
    torch.save(sam_refined_pred, save_file_path)


def save_all_SRMR_from_path(args):
    """
    Save SRMR maps for all feature files in a specified root directory.
    :param args: Arguments containing input path and GPU ID.
    :return: None
    """
    device = torch.device(f"cuda:{args.gpu_id}" if torch.cuda.is_available() else "cpu")

    image_paths = sorted(glob.glob(f"{args.input_path}/*.jpg"))
    print(f"Found {len(image_paths)} images in {args.input_path}")

    for image_path in tqdm(image_paths, desc="Processing feature files"):
        
        
        save_SRMR(clip_vis_feature, clip_text_features, sam2_masks, image_path.stem, args)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Extract SAM2 masks from images")
    parser.add_argument("--input_path", type=str, required=True, help="Path to input images directory")
    parser.add_argument("--save_path", type=str, default="", help="Path to save output masks")
    parser.add_argument("--gpu_id", type=int, default=0, help="GPU ID to use for computation")
    args = parser.parse_args()

    get_sam2_masks_from_path(args)

    print(f"Processing complete.")