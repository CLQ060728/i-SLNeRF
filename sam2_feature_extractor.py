# Author: Qian Liu
# Email: liu.qian.pro@gmail.com

import torch, numpy as np
import torch.nn.functional as F
from sam2.build_sam import build_sam2
from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator
import argparse
from tqdm import tqdm
from PIL import Image
from PIL.Image import Resampling
import cv2
from pathlib import Path
import glob, json, os
from torch import Tensor


def get_sam2_masks(image, device='cuda:0'):
    """
    Get SAM2 masks for a given image.
    :param image: Input image as a numpy array, (H, W, C).
    :param device: GPU device to use for computation.
    :return: List of masks generated by SAM2.
    """
    sam2_checkpoint = "checkpoints/sam2.1_hiera_large.pt"
    # sam2_checkpoint = "checkpoints/sam2.1_hiera_base_plus.pt"
    model_cfg = "configs/sam2.1/sam2.1_hiera_l.yaml"
    # model_cfg = "configs/sam2.1/sam2.1_hiera_b+.yaml"

    sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)

    # mask_generator = SAM2AutomaticMaskGenerator(sam2)
    mask_generator = SAM2AutomaticMaskGenerator(
        model=sam2,
        points_per_side=128,
        points_per_batch=128,
        pred_iou_thresh=0.7,
        stability_score_thresh=0.92,
        stability_score_offset=0.7,
        crop_n_layers=1,
        box_nms_thresh=0.7,
        crop_n_points_downscale_factor=2,
        min_mask_region_area=25.0,
        use_m2m=True,
    )

    masks = mask_generator.generate(image)
    masks = [mask['segmentation'] for mask in masks]  # Extract only the segmentation masks
    masks = torch.tensor(masks)  # Convert to tensor

    del sam2
    del mask_generator
    torch.cuda.empty_cache()

    return masks


def get_sam2_masks_from_path(args):
    """
    Get SAM2 masks for images in a specified directory.
    :param args: Arguments containing input path and GPU ID.
    :return: List of masks for each image.
    """
    device = torch.device(f"cuda:{args.gpu_id}" if torch.cuda.is_available() else "cpu")

    image_paths = sorted(glob.glob(f"{args.input_path}/*.jpg"))
    print(f"Found {len(image_paths)} images in {args.input_path}")
    # save the extracted masks  'sam2_masks'
    save_path = args.save_path
    os.makedirs(save_path, exist_ok=True)

    for image_path in tqdm(image_paths, desc="Processing images"):
        image_path = Path(image_path)
        image = Image.open(image_path).convert("RGB")
        print(f"Processing image: {image_path.name}, shape: {image.size}")
        # image = cv2.resize(image, (image.shape[1] // args.downscale, image.shape[0] // args.downscale),
                        #    interpolation=cv2.INTER_LINEAR)
        image = image.resize((image.width // args.downscale, image.height // args.downscale),
                            resample=Resampling.BILINEAR)
        print(f"Resized image: {image.size}")
        image = np.array(image)  # Convert to numpy array
        masks = get_sam2_masks(image, device=device)
        
        print(f"Extracted masks from {image_path.name}, shape: {masks.size()}, dtype: {masks.dtype}")
        save_file_path = os.path.join(save_path, f"{image_path.stem}.pt")
        torch.save(masks, save_file_path)


def save_SRMR(clip_vis_feature: Tensor, clip_text_features: Tensor, sam2_masks: Tensor, save_name, args):
    """
    Save the SRMR (Semantic Relevancy Map Regularization) map for a given image feature and
    its related scene classes features.
    :param vis_feature: visual features of the image (H, W, D).
    :param clip_text_features: CLIP text features of the scene classes.
    :param sam2_masks: SAM2 masks for the image.
    :param save_name: Name to save the SRMR map.
    :param args: Arguments containing GPU ID and other configurations.
    :return: A tensor of shape (H, W) with the regularized relavancy map of each pixel.
    """
    H, W =  clip_vis_feature.size(0), clip_vis_feature.size(1) # [height, width]
    device = clip_vis_feature.device
    
    # Get Clip features 
    clip_vis_feature = clip_vis_feature.reshape(-1, clip_vis_feature.size(-1)) # [N1, D], N1 is H*W, D is the feature dimension
    clip_text_features_normalized = F.normalize(clip_text_features, dim=1) # [N2, D], N2 is the number of scene classes
    clip_vis_feature_normalized = F.normalize(clip_vis_feature, dim=1) # [N1, D], N1 is 1 or H*W, D is the feature dimension
    # Compute cosine similarity
    relevancy_map = torch.mm(clip_vis_feature_normalized, clip_text_features_normalized.T) # [N1,N2]        
    p_class = F.softmax(relevancy_map, dim=1) # [N1,N2]
    class_index = torch.argmax(p_class, dim=-1) # [N1]
    pred_index = class_index.reshape(H, W).unsqueeze(0) # [1,H,W]

    # Refine SAM2 masks using the predicted class_index  
    sam_refined_pred = torch.zeros((pred_index.shape[1], pred_index.shape[2]),
                                   dtype=torch.long).to(device)

    for ann in sam2_masks:
        cur_mask = ann.squeeze()  # [H, W], current mask for the annotation                  
        sub_mask = pred_index.squeeze().clone()
        sub_mask[~cur_mask] = 0
        # .view(-1) collapses all dimensions into a single dimension, It is equivalent to tensor.reshape(-1).
        flat_sub_mask = sub_mask.clone().view(-1)           
        flat_sub_mask = flat_sub_mask[flat_sub_mask!=0]
        
        if len(flat_sub_mask) != 0:                         
            unique_elements, counts = torch.unique(flat_sub_mask, return_counts=True)  
            most_common_element = unique_elements[int(counts.argmax().item())]  
        else:                                               
            continue 

        sam_refined_pred[cur_mask] = most_common_element  
    
    # save the extracted masks 'srmr_masks'
    save_path = args.save_path
    os.makedirs(save_path, exist_ok=True)
    save_file_path = os.path.join(save_path, f"{save_name}.pt")
    torch.save(sam_refined_pred, save_file_path)


def save_all_SRMR_from_path(args):
    """
    Save SRMR maps for all feature files in a specified root directory.
    :param args: Arguments containing input path and GPU ID.
    :return: None
    """
    device = torch.device(f"cuda:{args.gpu_id}" if torch.cuda.is_available() else "cpu")

    clip_features_path = os.path.join(args.input_path, "clip_features")
    sam2_masks_path = os.path.join(args.input_path, "sam2_masks")
    clip_feature_paths = sorted(glob.glob(f"{clip_features_path}/*.pt"))
    print(f"Found {len(clip_feature_paths)} CLIP features in {clip_features_path}")
    
    # Load CLIP text features
    clip_text_features = torch.load(os.path.join(clip_features_path, "scene_classes_features.pt"),
                                    weights_only=True).to(device)

    for clip_feature_file_path in tqdm(clip_feature_paths, desc="Processing feature files"):
        save_name = Path(clip_feature_file_path).stem
        if save_name == "scene_classes_features":
            continue
        clip_vis_feature = torch.load(clip_feature_file_path, weights_only=True).to(device)  # [S, H, W, D]
        print(f"Processing feature file: {clip_feature_file_path}, shape: {clip_vis_feature.size()}")
        scale_index = torch.randint(0, clip_vis_feature.size(0), (1,))[0]  # Randomly select a scale index
        print(f"Selected scale index: {scale_index}")
        clip_vis_feature = clip_vis_feature[scale_index]  # [H, W, D]
        print(f"Selected feature shape: {clip_vis_feature.size()}")
        sam2_masks_file_path = os.path.join(sam2_masks_path, f"{save_name}.pt")
        sam2_masks = torch.load(sam2_masks_file_path, weights_only=True).to(device)  # [N, H, W]
        save_SRMR(clip_vis_feature, clip_text_features, sam2_masks, save_name, args)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Extract SAM2 masks from images")
    parser.add_argument("--input_path", type=str, required=True, help="Path to input images directory")
    parser.add_argument("--downscale", type=int, default=2, help="Downscale factor for image size")
    parser.add_argument("--save_path", type=str, default="", help="Path to save output masks")
    parser.add_argument("--gpu_id", type=int, default=0, help="GPU ID to use for computation")
    args = parser.parse_args()

    # get_sam2_masks_from_path(args)
    # save_all_SRMR_from_path(args)
    print(f"Processing complete.")