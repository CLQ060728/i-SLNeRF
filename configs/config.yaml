data:
  data_root: [.../data/...] # absolute path to the dataset
  aabb_type: specified # different types of aabbs, choose from ["front", "specified"]
  aabb: [-160.0, -160.0, -20.0, 160.0, 160.0, 60.0] # [-80.0, -80.0, -5.0, 80.0, 80.0, 7.8] size: [0.1m, 0.1m, 0.2m] and size: [0.4m, 0.4m, 0.4m]
                                              # [-40.0, -40.0, -1.0, 40.0, 40.0, 5.4] for Waymo
                                              # size: [0.4m, 0.4m, 0.4m]
                                              # [-40.0, -40.0, -1.0, 40.0, 40.0, 5.4] for NuScenes
                                              # size: [0.1m, 0.1m, 0.1m] - [-100.0, -100.0, -10.0, 100.0, 100.0, 10.0]
                                              # for Waymo
                                              # size: [0.4m, 0.4m, 0.4m] - [-120.0, -120.0, -20.0, 120.0, 120.0, 60.0]
                                              # for Waymo and NuScenes, the pre-defined aabb for the scene
  ray_batch_size: 8192 # ray batch size for training, it's embedded in the dataset class for now
  pixel_source: # everything related to "pixels" --- from camera images
    load_size:  [640, 960] # [height, width], resize the raw image to this size [640, 960] [1280, 1920]
    num_cams: 5 # number of cameras to use, choose from [1, 3, 5] for waymo, [1, 3, 6] for nuscenes. 1: frontal, 3: frontal + frontal_left + frontal_right
    load_rgb: True  # whether to load rgb images
    load_sky_mask: False  # whether to load sky masks. We provide pre-extracted sky masks for waymo and nuscenes on google drive
    load_depth_map: True # whether to load depth maps. We provide pre-extracted depth maps for waymo and nuscenes
    load_segmentation: True # whether to load segmentation masks. We provide pre-extracted segmentation masks for waymo and nuscenes
    test_image_stride: 0 # use every Nth timestep for the test set. if 0, use all images for training and none for testing

  lidar_source: # everything related to "lidar" --- from lidar points
    load_lidar: False

nerf:
  unbounded: True # use unbounded contraction as in mipnerf360 / merf / emernerf / distillnerf
  contract_method: inner_outer # choose contraction methods from ["inner_outer", "aabb_bounded"]
  inner_range: [80.0 ,80.0 ,20.0] # 0.4 - [80.0 ,80.0 ,20.0] - [240, 80]
                                 # 0.1 - [-50.0, -50.0, -10.0, 50.0, 50.0, 10.0] - [100, 50]
                                 # inner range for the unbounded contraction
  contract_ratio: 0.6 # how much to contract the aabb, 0.8 means 80% of the aabb

  propnet: # proposal networks hyperparameters
    num_samples_per_prop: [128, 64] # how many samples to use for each propnet
    near_plane: 0.1 # near plane for the propnet
    far_plane: 1000.0 # far plane for the propnet

  sampling: # sampling hyperparameters
    num_samples: 128 # final number of samples used for querying i-SLNeRF
  # sampling strategy, for aabb_bounded, we can use [128, 64], [200, 100], [240, 120] for propnet
  # 128, 200, 240 for i-SLNeRF;
  model:
    neck:
      base_mlp_layer_width: 64  # 128 fine tuning point
      geometry_feature_dim: 128 # 256 64 fine tuning point
      # ======= segmentations neck ======= #
      segmentation_feature_dim: 128 # 256 128 - when split_semantic_instance is True;
                                    # 128 64 - when split_semantic_instance is False;
    head:
      head_mlp_layer_width: 128 # 256 64 fine tuning point
      # ========== segmentations ======== #
      enable_segmentation_head: True # whether to use segmentation head
      split_semantic_instance: True # whether to use separate head for semantic and instance segmentation optimization
      semantic_hidden_dim: 128 # 256 hidden dimension for semantic segmentation head
      instance_hidden_dim: 128 # 256 hidden dimension for instance segmentation head
      semantic_embedding_dim: 128 # semantic embedding dimension
      instance_embedding_dim: 128 # 256 instance embedding dimension
      momentum: 0.9 # momentum for the slow instance network
      # ======= appearance embedding ======= #
      enable_cam_embedding: False # whether to use camera embedding, for novel view synthesis
      enable_img_embedding: True # whether to use image embedding, for scene reconstruction
      appearance_embedding_dim: 16 # appearance embedding dimension for each camera or image

      enable_sky_head: False # whether to use sky head
      enable_dynamic_branch: True # whether to use dynamic branch
      enable_shadow_head: True # whether to use shadow head to predict shadow ratio
      enable_flow_branch: True # whether to use flow branch

render:
  render_low_res: False # whether to render low resolution images

supervision:
  vision_depth: # vision depth supervision
    loss_type: l2 # choose from ["l1", "smooth_l1", "l2"]
    loss_coef: 1.0
    max_depth: 160.0 # max depth for the vision depth normalization
  segmentation: # semantic and instance supervision
    semantic:
      loss_coef: 0.1 # semantic loss coefficient
      start_iter: 10000 # when to start using semantic supervision
    instance:
      loss_coef: 0.1 # instance loss coefficient
      start_iter: 20000 # when to start using instance consistency supervision
optim: # optimization hyperparameters
  num_iters: 30000 # number of iterations to train

logging:
  vis_freq: 4000 # how often to visualize training stats
  print_freq: 200 # how often to print training stats
  saveckpt_freq: 4000 # how often to save checkpoints
  save_seperate_video: True # whether to save seperate video for each rendered key
resume_from: null # path to a checkpoint to resume from